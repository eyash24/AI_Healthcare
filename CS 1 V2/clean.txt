The following models are being compared for the classification task:
- Logistic Regression: A fundamental linear model that estimates the probability of a binary outcome using the logistic function. It serves as a strong baseline for classification problems due to its simplicity and interpretability.

- Decision Tree Classifier: A non-linear model that splits the data based on feature conditions, forming a tree-like structure of decisions. It is highly interpretable but can easily overfit without pruning or depth control.

- Random Forest Classifier: An ensemble of decision trees where each tree is trained on random subsets of data and features. It improves accuracy, generalization, and reduces overfitting compared to a single decision tree.

- Gradient Boosting Classifier: An ensemble technique that builds trees sequentially, where each tree learns from the errors of the previous one. It produces strong predictive performance and handles complex data patterns well.

- AdaBoost Classifier: An adaptive boosting algorithm that combines multiple weak learners, giving higher weights to misclassified samples in each round. It is effective for improving performance on difficult-to-classify cases.

- K-Nearest Neighbors (KNN): A distance-based model that assigns a class label based on the majority vote of its closest neighbors. Simple and effective, but sensitive to the choice of `k` and feature scaling.

- Support Vector Classifier (SVC): A model that finds the optimal hyperplane separating classes with maximum margin. With kernel functions, it can handle non-linear decision boundaries and is robust in high-dimensional spaces.

- Naïve Bayes (GaussianNB & BernoulliNB): Probabilistic classifiers based on Bayes’ theorem. GaussianNB assumes continuous features follow a normal distribution, while BernoulliNB is suited for binary features. Both are fast and efficient.

- Multi-Layer Perceptron (MLP): A type of neural network consisting of multiple fully connected layers. It captures complex non-linear relationships but requires more data and tuning compared to simpler models.

Performance metrics evaluated: Accuracy, Precision, Recall, F1-score, ROC-AUC score, and Confusion Matrix.

